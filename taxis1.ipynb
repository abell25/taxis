{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from random import shuffle, random\n",
    "from sklearn.cross_validation import ShuffleSplit, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def HaversineDistance(c1, c2): \n",
    "  lon_diff = np.abs(c1[0]-c2[0])*np.pi/360.0\n",
    "  lat_diff = np.abs(c1[1]-c2[1])*np.pi/360.0\n",
    "  a = np.sin(lat_diff)**2 + np.cos(c1[1]*np.pi/180.0) * np.cos(c2[1]*np.pi/180.0) * np.sin(lon_diff)**2\n",
    "  d = 2*6371*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "  return d\n",
    "\n",
    "def EuclidDistance(c1, c2):\n",
    "    return np.sqrt((c1[0]-c2[0])**2 + (c1[1]-c2[1])**2)\n",
    "\n",
    "\n",
    "def load_data(num_records_to_load=10):\n",
    "    submission_df = pd.read_csv('/home/tony/ML/taxi/taxi2_time/test.csv')\n",
    "    submission_df['POLYLINE'] = submission_df['POLYLINE'].apply(json.loads)\n",
    "    submission_df['COORDS_LEN'] = submission_df['POLYLINE'].apply(len)\n",
    "    submission_df['START'] = submission_df['POLYLINE'].apply(lambda x: x[0])\n",
    "\n",
    "    # read train\n",
    "    taxi_df = pd.read_csv('/home/tony/ML/taxi/taxi2_time/train.csv', nrows=num_records_to_load)\n",
    "    taxi_df['POLYLINE'] = taxi_df['POLYLINE'].apply(json.loads)\n",
    "    taxi_df['COORDS_LEN'] = taxi_df['POLYLINE'].apply(len)\n",
    "    taxi_df = taxi_df[taxi_df.COORDS_LEN > 10]\n",
    "    taxi_df['START'] = taxi_df['POLYLINE'].apply(lambda x: x[0])\n",
    "    taxi_df['END'] = taxi_df['POLYLINE'].apply(lambda x: x[-1])\n",
    "    \n",
    "    return taxi_df, submission_df\n",
    "\n",
    "\n",
    "def createTrainTestSplit(df, percent_test=0.1):\n",
    "    num_rows = len(df)\n",
    "    num_train = num_rows - int(num_rows*percent_test)\n",
    "    mask = np.random.rand(num_rows) > percent_test\n",
    "    \n",
    "    train_df, test_df = df[mask], df[~mask]\n",
    "    train_time, test_time = 15*train_df['COORDS_LEN'].values, 15*test_df['COORDS_LEN'].values\n",
    "    train_end, test_end = train_df['END'].values, test_df['END'].values\n",
    "\n",
    "    # Save reference to complete path for analytical purposes\n",
    "    test_df['POLYLINE_ACTUAL'] = test_df['POLYLINE'].values[:]\n",
    "    \n",
    "    #Create partial paths for the test data\n",
    "    #coords = test_df['POLYLINE'].values[:]\n",
    "    #partial_lengths = [round(0.5*len(coord)) for coord in coords]\n",
    "    #test_df['POLYLINE'] = [coords[:n] for n in partial_lengths]\n",
    "    test_df['POLYLINE'] = [coord[:int(round(0.5*len(coord)))] for coord in test_df['POLYLINE'].values]\n",
    "    \n",
    "    \n",
    "    #Drop all data that we shouldn't have during training\n",
    "    test_df = test_df.drop(['COORDS_LEN', 'END'], axis=1)\n",
    "    test_df['COORDS_LEN'] = test_df['POLYLINE'].apply(len)\n",
    "    \n",
    "    return train_df, test_df, train_time, test_time, train_end, test_end\n",
    "\n",
    "def travelTimeScore(pred_times, actual_times):\n",
    "    score = np.sqrt(np.mean((np.log(pred_times+1)-np.log(actual_times+1))**2))\n",
    "    return score\n",
    "\n",
    "def travelEndScore(pred_ends, actual_ends):\n",
    "    num_points = len(pred_ends)\n",
    "    preds, actuals = pred_ends, actual_ends\n",
    "    score = np.mean([HaversineDistance(preds[i], actuals[i]) for i in range(num_points)])\n",
    "    return score\n",
    "    \n",
    "def submitTravelTime(validation_df, filename):\n",
    "    validation_df[['TRIP_ID', 'TRAVEL_TIME']].to_csv(filename, index=False)\n",
    "    \n",
    "def submitTravelDestination(validation_df, filename):\n",
    "    validation_df['LATITUDE'] = validation_df['TRAVEL_END'].apply(lambda x: x[1])\n",
    "    validation_df['LONGITUDE'] = validation_df['TRAVEL_END'].apply(lambda x: x[0])\n",
    "    validation_df[['TRIP_ID', 'LATITUDE', 'LONGITUDE']].to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weightedMeanTravelTimes(train, test, num_trips=100):\n",
    "    test['TRAVEL_TIME'] = 0\n",
    "    for idx, start_coord in enumerate(test['START']):\n",
    "        dists = train['START'].apply(lambda x: HaversineDistance(x, start_coord))\n",
    "        smallest_dist_indexes = np.argpartition(dists, num_trips)[0:num_trips]\n",
    "        w = np.maximum(dists.iloc[smallest_dist_indexes], 0.01)\n",
    "        path_lengths = train.iloc[smallest_dist_indexes]['COORDS_LEN']\n",
    "        no_path_lengths_indexes = np.argpartition(path_lengths, int(num_trips*.95))[0:int(num_trips*.95)]\n",
    "        print idx\n",
    "        #print \"test[idx, travel_time]: \", test[idx, 'TRAVEL_TIME']\n",
    "        print \"test.loc(idx, coords_len): \", test.loc[idx, \"COORDS_LEN\"]\n",
    "        print \"avg: \", np.average(s.iloc[no_path_lengths_indexes], weights=1/w.iloc[no_path_lengths_indexes]**2)\n",
    "        #test.loc[idx, 'TRAVEL_TIME'] = 15*np.maximum(test.loc[idx, 'COORDS_LEN'], np.average(s.iloc[no_path_lengths_indexes], weights=1/w.iloc[no_path_lengths_indexes]**2))\n",
    "        test.loc[idx, 'TRAVEL_TIME'] = 15*np.maximum(test.loc[idx, 'COORDS_LEN'], np.average(path_lengths.iloc[no_path_lengths_indexes], weights=1/w.iloc[no_path_lengths_indexes]**2))\n",
    "\n",
    "        \n",
    "    test['TRAVEL_TIME'] = test['TRAVEL_TIME'].astype(int)\n",
    "    return test['TRAVEL_TIME'].values\n",
    "\n",
    "\n",
    "def weightedMeanTravelTimes3(train, test, num_trips=100):\n",
    "    test_times = np.zeros(len(test))\n",
    "    for idx, start_coord in enumerate(test['START'].values):\n",
    "        dists = np.array([HaversineDistance(x, start_coord) for x in train['START'].values])\n",
    "        smallest_dist_indexes = np.argpartition(dists, num_trips)[0:num_trips]\n",
    "        w = np.maximum(dists[smallest_dist_indexes], 0.01)\n",
    "        path_lengths = train['COORDS_LEN'].values[smallest_dist_indexes]\n",
    "        no_path_lengths_indexes = np.argpartition(path_lengths, int(num_trips*.95))[0:int(num_trips*.95)]\n",
    "        test_times[idx] = 15*np.maximum(test['COORDS_LEN'].values[idx], np.average(path_lengths[no_path_lengths_indexes], weights=1/w[no_path_lengths_indexes]**2))\n",
    "\n",
    "        \n",
    "    test['TRAVEL_TIME'] = test_times.astype(int)\n",
    "    return test_times\n",
    "\n",
    "\n",
    "# good score! 0.385 locally, 0.556 on leaderboard for 20k examples, 4 num_trips\n",
    "def weightedMeanTravelTimes_2_Points(train, test, num_trips=100):\n",
    "    test_times = np.zeros(len(test))\n",
    "    train_lens = train['POLYLINE'].apply(len)\n",
    "    for idx, test_coord in enumerate(test['POLYLINE'].values):\n",
    "        dists_starts = np.array([HaversineDistance(x[0], test_coord[0]) for x in train['POLYLINE'].values])\n",
    "        dists_ends = np.array([(HaversineDistance(x[(len(test_coord)-1)], test_coord[-1]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists = dists_starts + dists_ends\n",
    "        smallest_dist_indexes = np.argpartition(dists, num_trips)[0:num_trips]\n",
    "        w = np.maximum(dists[smallest_dist_indexes], 0.01)\n",
    "        path_lengths = train['COORDS_LEN'].values[smallest_dist_indexes]\n",
    "        no_path_lengths_indexes = np.argpartition(path_lengths, int(num_trips*.95))[0:int(num_trips*.95)]\n",
    "        test_times[idx] = 15*np.maximum(test['COORDS_LEN'].values[idx], np.average(path_lengths[no_path_lengths_indexes], weights=1/w[no_path_lengths_indexes]**2))\n",
    "\n",
    "    test['TRAVEL_TIME'] = test_times.astype(int)\n",
    "    return test_times\n",
    "        \n",
    "def weightedMeanTravelTimes_3_Points(train, test, num_trips=100):\n",
    "    test_times = np.zeros(len(test))\n",
    "    train_lens = train['POLYLINE'].apply(len)\n",
    "    for idx, test_coord in enumerate(test['POLYLINE'].values):\n",
    "        dists_starts = np.array([HaversineDistance(x[0], test_coord[0]) for x in train['POLYLINE'].values])\n",
    "        dists_mids = np.array([(HaversineDistance(x[(int(len(test_coord)/2))], test_coord[int(len(test_coord)/2)]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists_ends = np.array([(HaversineDistance(x[(len(test_coord)-1)], test_coord[-1]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists = dists_starts + dists_mids + dists_ends\n",
    "        smallest_dist_indexes = np.argpartition(dists, num_trips)[0:num_trips]\n",
    "        w = np.maximum(dists[smallest_dist_indexes], 0.01)\n",
    "        path_lengths = train['COORDS_LEN'].values[smallest_dist_indexes]\n",
    "        no_path_lengths_indexes = np.argpartition(path_lengths, int(num_trips*.95))[0:int(num_trips*.95)]\n",
    "        test_times[idx] = 15*np.maximum(test['COORDS_LEN'].values[idx], np.average(path_lengths[no_path_lengths_indexes], weights=1/w[no_path_lengths_indexes]**2))\n",
    "     \n",
    "    test['TRAVEL_TIME'] = test_times.astype(int)\n",
    "    return test_times\n",
    "\n",
    "\n",
    "def weightedMeanTravelEnd_3_Points(train, test, num_trips=10):\n",
    "    pred_ends = [[0,0] for x in range(len(test['POLYLINE'].values)) ]\n",
    "    train_ends = train['END'].values\n",
    "    train_lens = train['POLYLINE'].apply(len)\n",
    "    for idx, test_coord in enumerate(test['POLYLINE'].values):\n",
    "        dists_starts = np.array([HaversineDistance(x[0], test_coord[0]) for x in train['POLYLINE'].values])\n",
    "        dists_mids = np.array([(HaversineDistance(x[(int(len(test_coord)/2))], test_coord[int(len(test_coord)/2)]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists_ends = np.array([(HaversineDistance(x[(len(test_coord)-1)], test_coord[-1]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists = dists_starts + dists_mids + dists_ends\n",
    "        smallest_dist_indexes = np.argpartition(dists, num_trips)[0:num_trips]\n",
    "        \n",
    "        w = np.maximum(dists[smallest_dist_indexes], 0.01)\n",
    "        path_lengths = train['COORDS_LEN'].values[smallest_dist_indexes]\n",
    "        no_path_lengths_indexes =  np.argpartition(path_lengths, int(num_trips*.95))[0:int(num_trips*.95)]\n",
    "        \n",
    "        points = train_ends[no_path_lengths_indexes] \n",
    "        dists_sum = [0 for _ in range(len(points))]\n",
    "        for k in range(len(points)):\n",
    "            dists_sum[k] = np.sum([1/max(0.001, EuclidDistance(points[c], points[k])**2) for c in range(len(points)) if c != k])\n",
    "        densest = points[np.argmax(dists_sum)]\n",
    "        pred_ends[idx] = densest\n",
    "    \n",
    "    test['TRAVEL_END'] = pred_ends\n",
    "    return pred_ends\n",
    "\n",
    "def weightedMeanTravelEnd_3_Points_focusOnEnd(train, test, num_trips=10):\n",
    "    pred_ends = [[0,0] for x in range(len(test['POLYLINE'].values)) ]\n",
    "    train_ends = train['END'].values\n",
    "    train_lens = train['POLYLINE'].apply(len)\n",
    "    for idx, test_coord in enumerate(test['POLYLINE'].values):\n",
    "        #dists_starts = np.array([HaversineDistance(x[0], test_coord[0]) for x in train['POLYLINE'].values])\n",
    "        dists_4 = np.array([(HaversineDistance(x[(int(len(test_coord)-min(len(test_coord), 4)))], test_coord[int(len(test_coord)-min(len(test_coord), 4))]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists_3 = np.array([(HaversineDistance(x[(int(len(test_coord)-min(len(test_coord), 3)))], test_coord[int(len(test_coord)-min(len(test_coord), 3))]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists_2 = np.array([(HaversineDistance(x[(int(len(test_coord)-min(len(test_coord), 2)))], test_coord[int(len(test_coord)-min(len(test_coord), 2))]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists_1 = np.array([(HaversineDistance(x[(len(test_coord)-1)], test_coord[-1]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists =  0.9*dists_1 + 0.8*dists_2 + 0.7*dists_3 + 0.6*dists_4\n",
    "        \n",
    "        \n",
    "        smallest_dist_indexes = np.argpartition(dists, num_trips)[0:num_trips]\n",
    "        \n",
    "        w = np.maximum(dists[smallest_dist_indexes], 0.01)\n",
    "        path_lengths = train['COORDS_LEN'].values[smallest_dist_indexes]\n",
    "        no_path_lengths_indexes =  np.argpartition(path_lengths, int(num_trips*.95))[0:int(num_trips*.95)]\n",
    "        \n",
    "        points = train_ends[no_path_lengths_indexes] \n",
    "        dists_sum = [0 for _ in range(len(points))]\n",
    "        for k in range(len(points)):\n",
    "            dists_sum[k] = np.sum([1/max(0.01, EuclidDistance(points[c], points[k])**2) for c in range(len(points)) if c != k])\n",
    "        densest = points[np.argmax(dists_sum)]\n",
    "        pred_ends[idx] = densest\n",
    "    \n",
    "    test['TRAVEL_END'] = pred_ends\n",
    "    return pred_ends\n",
    "\n",
    "\n",
    "def weightedMeanTravelEnd_3_Points_with_w(train, test, num_trips=10):\n",
    "    pred_ends = [[0,0] for x in range(len(test['POLYLINE'].values)) ]\n",
    "    train_ends = train['END'].values\n",
    "    train_lens = train['POLYLINE'].apply(len)\n",
    "    for idx, test_coord in enumerate(test['POLYLINE'].values):\n",
    "        dists_starts = np.array([HaversineDistance(x[0], test_coord[0]) for x in train['POLYLINE'].values])\n",
    "        dists_mids = np.array([(HaversineDistance(x[(int(len(test_coord)/2))], test_coord[int(len(test_coord)/2)]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists_ends = np.array([(HaversineDistance(x[(len(test_coord)-1)], test_coord[-1]) if len(x) > len(test_coord) else 100) for x in train['POLYLINE'].values ])\n",
    "        dists = dists_starts + dists_mids + dists_ends\n",
    "        smallest_dist_indexes = np.argpartition(dists, num_trips)[0:num_trips]\n",
    "        \n",
    "        w = np.maximum(dists[smallest_dist_indexes], 0.01)\n",
    "        path_lengths = train['COORDS_LEN'].values[smallest_dist_indexes]\n",
    "        no_path_lengths_indexes =  np.argpartition(path_lengths, int(num_trips*.95))[0:int(num_trips*.95)]\n",
    "        \n",
    "        points = train_ends[no_path_lengths_indexes] \n",
    "        w_points = w[no_path_lengths_indexes]\n",
    "        dists_sum = [0 for _ in range(len(points))]\n",
    "        for k in range(len(points)):\n",
    "            dists_sum[k] = w_points[k]*np.sum([1/max(0.01, EuclidDistance(points[c], points[k])**2) for c in range(len(points)) if c != k])\n",
    "        densest = points[np.argmax(dists_sum)]\n",
    "        pred_ends[idx] = densest\n",
    "    \n",
    "    test['TRAVEL_END'] = pred_ends\n",
    "    return pred_ends\n",
    "\n",
    "class PointClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y, num_records=4000, num_test=200):\n",
    "        train_df, test_df, train_time, test_time, train_end, test_end = createTrainTestSplit(taxi_df, num_test/float(num_records))\n",
    "\n",
    "    \n",
    "    def predict(self, X, y, k_neighbors=5):\n",
    "        pred_time = weightedMeanTravelTimes_3_Points(train_df, test_, k_neighbors)\n",
    "        return travelTimeScore(pred_time, test_time)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(np.array([1,2,3,4,5]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11_23'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'%d_%d' % (11,23.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(num_records=1000, num_averaged=10, cv_folds=10, cv_size=20, submit=False, verbose=False):\n",
    "    taxi_df, submission_df = load_data(num_records_to_load=num_records)\n",
    "\n",
    "    scores = []\n",
    "    for k in range(cv_folds):\n",
    "        train_df, test_df, train_time, test_time, train_end, test_end = createTrainTestSplit(taxi_df, cv_size/float(num_records))\n",
    "        #print \"num training:\", len(train_df), \"   num test:\", len(test_df)\n",
    "\n",
    "        pred_time = weightedMeanTravelTimes_3_Points(train_df, test_df, num_averaged)\n",
    "        score = travelTimeScore(pred_time, test_time)\n",
    "        scores.append(score)\n",
    "        if verbose:\n",
    "            print \"score:\", score, \"average:\", np.average(scores)\n",
    "\n",
    "    print \"score:\", np.average(scores), \", num_records:\", num_records, \", num_averaged:\", num_averaged\n",
    "            \n",
    "    if submit:\n",
    "        validation_time = weightedMeanTravelTimes_3_Points(train_df, submission_df, num_averaged)\n",
    "        submitTravelTime(submission_df, 'sub_time__%d_%d_%d.csv' % (num_records, num_averaged, np.average(scores)*100))\n",
    "\n",
    "for num_averaged in [5, 25, 100, 1000]:\n",
    "    for num_records in [500000, 1000000]:\n",
    "        train(num_records, num_averaged, cv_folds=5, cv_size=100, submit=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_dest(num_records=1000, num_averaged=10, cv_folds=10, cv_size=20, submit=False, verbose=False):\n",
    "    taxi_df, submission_df = load_data(num_records_to_load=num_records)\n",
    "\n",
    "    scores = []\n",
    "    for k in range(cv_folds):\n",
    "        train_df, test_df, train_time, test_time, train_end, test_end = createTrainTestSplit(taxi_df, cv_size/float(num_records))\n",
    "        #print \"num training:\", len(train_df), \"   num test:\", len(test_df)\n",
    "\n",
    "    pred_ends = weightedMeanTravelEnd_3_Points_focusOnEnd(train_df, test_df, num_averaged)\n",
    "    score = travelEndScore(pred_ends, test_end)\n",
    "    scores.append(score)\n",
    "    if verbose:\n",
    "        print \"score: \", score, \"avg: \", np.average(scores)\n",
    "\n",
    "    print \"avg: \", np.average(scores), \"min:\", np.min(scores), \"max:\", np.max(scores), \"num_records: \", num_records, \"num_averaged:\", num_averaged\n",
    "    \n",
    "    if submit:\n",
    "        validation_time = weightedMeanTravelEnd_3_Points_focusOnEnd(train_df, submission_df, num_samples)\n",
    "        submitTravelDestination(submission_df, 'sub_dest__%d_%d_%d.csv' % (num_records, num_averaged, np.average(scores)*100))\n",
    "        \n",
    "for num_records in [2000, 20000, 100000, 500000, 1000000]:\n",
    "    for num_averaged in [5, 15, 30, 80]:\n",
    "        train_dest(num_records, num_averaged, cv_folds=5, cv_size=100, submit=True, verbose=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(submission_df['COORDS_LEN'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-8.604594, 41.134158],\n",
       " [-8.604594, 41.134158],\n",
       " [-8.604594, 41.134158],\n",
       " [-8.66574, 41.170671],\n",
       " [-8.604594, 41.134158],\n",
       " [-8.6247, 41.161554],\n",
       " [-8.589402, 41.163309],\n",
       " [-8.603973, 41.142816],\n",
       " [-8.604594, 41.134158],\n",
       " [-8.604594, 41.134158],\n",
       " [-8.604594, 41.134158]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
